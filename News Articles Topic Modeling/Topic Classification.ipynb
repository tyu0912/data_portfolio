{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Topic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\tyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score, f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import nltk\n",
    "import difflib\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from sklearn import decomposition\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "word_list = words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (2034L,)\n",
      "test label shape: (677L,)\n",
      "dev label shape: (676L,)\n",
      "labels names: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "num_test = len(newsgroups_test.target)\n",
    "test_data, test_labels = newsgroups_test.data[num_test/2:], newsgroups_test.target[num_test/2:]\n",
    "dev_data, dev_labels = newsgroups_test.data[:num_test/2], newsgroups_test.target[:num_test/2]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print 'training label shape:', train_labels.shape\n",
    "print 'test label shape:', test_labels.shape\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "print 'labels names:', newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \n",
      "    Training data 1:\n",
      "    \n",
      "    Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "    \n",
      "    Training label 1:\n",
      "    \n",
      "    1\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    Training data 2:\n",
      "    \n",
      "    \n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.\n",
      "    \n",
      "    Training label 2:\n",
      "    \n",
      "    3\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    Training data 3:\n",
      "    \n",
      "    \n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      " \n",
      "    \n",
      "    Training label 3:\n",
      "    \n",
      "    2\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    Training data 4:\n",
      "    \n",
      "    I have a request for those who would like to see Charley Wingate\n",
      "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
      "appear to be quite a few of you.)  \n",
      "\n",
      "It is clear that Mr. Wingate intends to continue to post tangential or\n",
      "unrelated articles while ingoring the Challenges themselves.  Between\n",
      "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
      "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
      "\n",
      "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
      "will just go away, and he is doing his level best to change the\n",
      "subject.  Given that this seems a rather common net.theist tactic, I\n",
      "would like to suggest that we impress upon him our desire for answers,\n",
      "in the following manner:\n",
      "\n",
      "1. Ignore any future articles by Mr. Wingate that do not address the\n",
      "Challenges, until he answers them or explictly announces that he\n",
      "refuses to do so.\n",
      "\n",
      "--or--\n",
      "\n",
      "2. If you must respond to one of his articles, include within it\n",
      "something similar to the following:\n",
      "\n",
      "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
      "\n",
      "Really, I'm not looking to humiliate anyone here, I just want some\n",
      "honest answers.  You wouldn't think that honesty would be too much to\n",
      "ask from a devout Christian, would you?  \n",
      "\n",
      "Nevermind, that was a rhetorical question.\n",
      "    \n",
      "    Training label 4:\n",
      "    \n",
      "    0\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    Training data 5:\n",
      "    \n",
      "    AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
      "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
      "\n",
      "Does anyone know more about this?  How much, to attend????\n",
      "\n",
      "Anyone want to go?\n",
      "    \n",
      "    Training label 5:\n",
      "    \n",
      "    2\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for x in range(0,5):\n",
    "    print \"\"\"\n",
    "    \n",
    "    Training data {0}:\n",
    "    \n",
    "    {1}\n",
    "    \n",
    "    Training label {0}:\n",
    "    \n",
    "    {2}\n",
    "    \n",
    "    \"\"\".format(str(x+1),train_data[x], train_labels[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple EDA\n",
    "\n",
    "Here I am pritting out some basic properties of the data including:\n",
    "- The size of the vocabulary\n",
    "- The average number of non-zero features\n",
    "- The fraction of the entries in the matrix are non-zero\n",
    "- The 0th and last feature strings\n",
    "- The average number of non-zero features per example\n",
    "- The bigram and trigram vocabulary size\n",
    "- Size of vocabulary does this yield after pruning < 10 words\n",
    "- Theraction of the words in the dev data that are missing from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part A\n",
      "\n",
      "The size of the vocabulary is 26879\n",
      "The average number of non-zero features per example:  96\n",
      "The percent (converted from the fraction) of the entries that are non-zero:  0.36 %\n",
      "\n",
      "Part B\n",
      "\n",
      "The 0th feature string is 00\n",
      "The last feature string is zyxel\n",
      "\n",
      "Part C\n",
      "\n",
      "The total number of non-zero elements are:  546\n",
      "The average number of non-zero features per example:  0.268436578171\n",
      "\n",
      "Part D\n",
      "\n",
      "The number of bigrams words are:  194891\n",
      "The number of trigrams words are:  315692\n",
      "The number of bigrams characters are:  3291\n",
      "The number of trigrams characters are:  32187\n",
      "\n",
      "Part E\n",
      "\n",
      "The number of vocab using min_df: 10 are:  3064\n",
      "\n",
      "Part F\n",
      "\n",
      "The percent of words in dev data missing from the vocab from the training data is (25.0, '%')\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(train_data)\n",
    "print \"The size of the vocabulary is\", len(vectorizer.get_feature_names())\n",
    "print \"The average number of non-zero features per example: \", X.nnz/X.shape[0]\n",
    "print \"The percent (converted from the fraction) of the entries that are non-zero: \", round(float(X.nnz)/(X.toarray().size)*100,2), '%'\n",
    "\n",
    "sorted_features = sorted(vectorizer.get_feature_names())\n",
    "\n",
    "print \"The 0th feature string is\", sorted_features[0]\n",
    "print \"The last feature string is\", sorted_features[len(vectorizer.get_feature_names())-1]\n",
    "\n",
    "vocab = [\"atheism\", \"graphics\", \"space\", \"religion\"]\n",
    "vectorizer = CountVectorizer(vocabulary = vocab)\n",
    "\n",
    "X = vectorizer.fit_transform(train_data)\n",
    "print \"The total number of non-zero elements are: \", X.nnz\n",
    "print \"The average number of non-zero features per example: \", float(X.nnz)/X.shape[0]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range = (2,2), analyzer='word')\n",
    "X = vectorizer.fit_transform(train_data)\n",
    "print 'The number of bigrams words are: ', len(vectorizer.get_feature_names())\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range = (3,3), analyzer='word')\n",
    "X = vectorizer.fit_transform(train_data)\n",
    "print 'The number of trigrams words are: ', len(vectorizer.get_feature_names())\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range = (2,2), analyzer='char')\n",
    "X = vectorizer.fit_transform(train_data)\n",
    "print 'The number of bigrams characters are: ', len(vectorizer.get_feature_names())\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range = (3,3), analyzer='char')\n",
    "X = vectorizer.fit_transform(train_data)\n",
    "print 'The number of trigrams characters are: ', len(vectorizer.get_feature_names())\n",
    "\n",
    "setting = 10\n",
    "vectorizer = CountVectorizer(min_df = setting)\n",
    "X = vectorizer.fit_transform(train_data)\n",
    "print 'The number of vocab using min_df:', setting ,'are: ', len(vectorizer.get_feature_names())\n",
    "\n",
    "def get_vocab_size(data):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    \n",
    "    return vectorizer.get_feature_names()\n",
    "\n",
    "in_dev_notin_train = np.setdiff1d(get_vocab_size(dev_data),get_vocab_size(train_data))\n",
    "per_miss_in_dev = round(1.0*len(in_dev_notin_train)/len(get_vocab_size(dev_data))*100),'%'\n",
    "\n",
    "print 'The percent of words in dev data missing from the vocab from the training data is', per_miss_in_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying some models\n",
    "\n",
    "Here I tried some quick hyperparameter tuning for knn, multinomial nb, and logistic regression and print our the performance metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data2df(train, dev, gram = (1,1), preprocessor = None, show_features = 0):\n",
    "    \n",
    "    if gram == (1,1):\n",
    "        print 'Normal vectorizer chosen'\n",
    "    else:\n",
    "        print 'Anagram vectorizer chosen:', gram\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range = gram, analyzer='word')\n",
    "        \n",
    "    if preprocessor != None:\n",
    "        print 'Adding preprocessing function.'\n",
    "        vectorizer.set_params(preprocessor=preprocessor)        \n",
    "        \n",
    "    train_matrix = vectorizer.fit_transform(train).todense()\n",
    "    dev_matrix = vectorizer.transform(dev).todense()\n",
    "    \n",
    "    columns = vectorizer.get_feature_names()\n",
    "    \n",
    "    df_train = pd.DataFrame(data=train_matrix, columns=columns)\n",
    "    df_dev = pd.DataFrame(data=dev_matrix, columns=columns)\n",
    "    \n",
    "    if show_features != 0:\n",
    "        print columns\n",
    "    \n",
    "    print \"Data prepared.\"\n",
    "    \n",
    "    return df_train, df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayse \n",
      "\n",
      "Optimal paramter: {'alpha': 0.01} \n",
      "\n",
      "Accuracy score: 0.7795857988165681 \n",
      "\n",
      "F1 score: 0.7751663218544357 \n",
      "\n",
      "Saving best parameter... 0.01 \n",
      "\n",
      "Logistic Regression \n",
      "\n",
      "Optimal paramter: {'C': 0.18000000000000002} \n",
      "\n",
      "Accuracy score: 0.7144970414201184 \n",
      "\n",
      "F1 score: 0.7075749110117433 \n",
      "\n",
      "Saving best parameter... 0.18000000000000002 \n",
      "\n",
      "K Nearest Neighbors \n",
      "\n",
      "Optimal paramter: {'n_neighbors': 97} \n",
      "\n",
      "Accuracy score: 0.47928994082840237 \n",
      "\n",
      "F1 score: 0.46655013143953317 \n",
      "\n",
      "Saving best parameter... 97 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_best_parameter(name, c,p):\n",
    "    \n",
    "    #scorer = make_scorer(accuracy_score)\n",
    "    scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "    grid_obj = GridSearchCV(c, p, scoring=scorer)\n",
    "    grid_obj = grid_obj.fit(train_data_df, train_labels)\n",
    "    \n",
    "    classifier = grid_obj.best_estimator_\n",
    "    classifier.fit(train_data_df, train_labels)\n",
    "    predictions = classifier.predict(dev_data_df)\n",
    "\n",
    "    print name,'\\n'\n",
    "    print 'Optimal paramter:', grid_obj.best_params_, '\\n'\n",
    "    print 'Accuracy score:', accuracy_score(dev_labels, predictions), '\\n'\n",
    "    print 'F1 score:', f1_score(dev_labels, predictions, average='weighted'), '\\n'\n",
    "    \n",
    "    best_param = grid_obj.best_params_[grid_obj.best_params_.keys()[0]]\n",
    "    \n",
    "    print 'Saving best parameter...',best_param, '\\n'\n",
    "    return best_param\n",
    "\n",
    "\n",
    "train_data_df, dev_data_df = vectorize_data2df(train_data,dev_data,(1,1))\n",
    "\n",
    "mnb_bestp = find_best_parameter('Multinomial Naive Bayse', MultinomialNB(), {'alpha': [0,0.01,0.001]})\n",
    "log_bestp = find_best_parameter('Logistic Regression', LogisticRegression(), {'C': np.arange(0.01,0.25,0.01)})\n",
    "knn_bestp = find_best_parameter('K Nearest Neighbors', KNeighborsClassifier(), {'n_neighbors': [97]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "0.01     2.541473       2.939709   2.862469            2.249805\n",
      "0.02     5.470644       5.876188   5.940800            4.668306\n",
      "0.03     8.398836       8.618777   8.926027            7.097120\n",
      "0.04    11.268958      11.206100  11.810930            9.497170\n",
      "0.05    14.073354      13.670579  14.604472           11.860673\n",
      "0.06    16.808967      16.026400  17.314791           14.178626\n",
      "0.07    19.479953      18.294667  19.957428           16.454128\n",
      "0.08    22.083626      20.481777  22.514227           18.683922\n",
      "0.09    24.633662      22.602634  25.020023           20.874869\n",
      "0.1     27.129775      24.655334  27.459466           23.024233\n",
      "0.11    29.576239      26.659901  29.844322           25.136564\n",
      "0.12    31.976483      28.592543  32.181850           27.220880\n",
      "0.13    34.327876      30.510160  34.469254           29.251986\n",
      "0.14    36.644318      32.371745  36.716731           31.252154\n",
      "0.15    38.919134      34.184769  38.908592           33.232385\n",
      "0.16    41.143050      35.964176  41.052471           35.179613\n",
      "0.17    43.327588      37.707490  43.185705           37.093324\n",
      "0.18    45.506524      39.420452  45.268842           38.980644\n",
      "0.19    47.652606      41.095723  47.303433           40.831313\n",
      "0.2     49.734306      42.748209  49.323294           42.668706\n",
      "0.21    51.823250      44.358942  51.303308           44.476791\n",
      "0.22    53.859338      45.957131  53.259540           46.249879\n",
      "0.23    55.868991      47.491288  55.181342           48.017122\n",
      "0.24    57.860556      49.071492  57.072042           49.750959\n"
     ]
    }
   ],
   "source": [
    "# Finding the sum of the squared weight values for each class for \n",
    "# each setting of the C parameter for logistic regression\n",
    "\n",
    "cls = LogisticRegression()\n",
    "parameters = np.arange(0.01,0.25,0.01)\n",
    "\n",
    "all_val_df = pd.DataFrame()\n",
    "\n",
    "for parameter in parameters:\n",
    "    cls.set_params(C = parameter)\n",
    "    cls.fit(train_data_df, train_labels)\n",
    "    \n",
    "    coefficients = (pd.DataFrame(cls.coef_))**2\n",
    "    all_val_df[parameter] = coefficients.sum(axis=1)\n",
    "    \n",
    "all_val_df = all_val_df.T\n",
    "all_val_df = all_val_df.rename(index=str, columns={i:v for i,v in enumerate(newsgroups_train.target_names)})\n",
    "\n",
    "ax = all_val_df.plot()\n",
    "ax.set(xlabel=\"C\", ylabel=\"Sum of Squared Weight\")\n",
    "plt.xticks(np.arange(len(all_val_df.index)), all_val_df.index, rotation=70)\n",
    "plt.show()\n",
    "\n",
    "print all_val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the top weighted words/bigrams for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "[(u'islam', 0.5121043477615559), (u'atheists', 0.5828907106939208), (u'religion', 0.5959915607989166), (u'atheism', 0.5965268297095336), (u'bobby', 0.5970158432272148)] \n",
      "\n",
      "comp.graphics\n",
      "[(u'computer', 0.660434568308712), (u'3d', 0.684595250809784), (u'file', 0.7819793535620624), (u'image', 0.8029676975621671), (u'graphics', 1.2157367157454406)] \n",
      "\n",
      "sci.space\n",
      "[(u'spacecraft', 0.5024902802321605), (u'launch', 0.581569621863684), (u'nasa', 0.6447253806899584), (u'orbit', 0.7358292117280132), (u'space', 1.4672869418707928)] \n",
      "\n",
      "talk.religion.misc\n",
      "[(u'order', 0.534064564567943), (u'fbi', 0.5366450550779899), (u'blood', 0.5706982208008935), (u'christians', 0.6408258201306598), (u'christian', 0.6723947059101167)] \n",
      "\n",
      "            alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "islam          0.512104      -0.092704  -0.205246           -0.206956\n",
      "atheists       0.582891      -0.086082  -0.195583           -0.399438\n",
      "religion       0.595992      -0.368029  -0.482990           -0.015880\n",
      "atheism        0.596527      -0.257989  -0.255634           -0.305556\n",
      "bobby          0.597016      -0.146206  -0.208700           -0.282845\n",
      "computer      -0.011246       0.660435  -0.411338           -0.283203\n",
      "3d            -0.224080       0.684595  -0.397770           -0.228931\n",
      "file          -0.207905       0.781979  -0.511834           -0.358943\n",
      "image         -0.329185       0.802968  -0.468188           -0.269731\n",
      "graphics      -0.488488       1.215737  -0.800375           -0.455304\n",
      "spacecraft    -0.215568      -0.230604   0.502490           -0.186741\n",
      "launch        -0.263950      -0.298350   0.581570           -0.207348\n",
      "nasa          -0.334322      -0.312844   0.644725           -0.304646\n",
      "orbit         -0.263066      -0.410338   0.735829           -0.329228\n",
      "space         -0.793085      -0.849364   1.467287           -0.716116\n",
      "order         -0.454506      -0.047047  -0.095540            0.534065\n",
      "fbi           -0.163203      -0.145415  -0.273907            0.536645\n",
      "blood         -0.282266      -0.076278  -0.135751            0.570698\n",
      "christians    -0.424818      -0.209480  -0.264672            0.640826\n",
      "christian     -0.330448      -0.239035  -0.217511            0.672395\n"
     ]
    }
   ],
   "source": [
    "cls = LogisticRegression(C = log_bestp)\n",
    "cls.fit(train_data_df, train_labels)\n",
    "\n",
    "coefficients = cls.coef_\n",
    "df_coef = pd.DataFrame(coefficients, columns = train_data_df.columns.values)\n",
    "\n",
    "best_cols = []\n",
    "\n",
    "for i,thelist in enumerate(coefficients):\n",
    "    print newsgroups_train.target_names[i]\n",
    "    \n",
    "    top_list = sorted(zip(train_data_df.columns.values, cls.coef_[i]),key=lambda tup: tup[1])[-5:]\n",
    "    print top_list,'\\n'\n",
    "    \n",
    "    for (name, value) in top_list:\n",
    "        best_cols.append(name)\n",
    "        \n",
    "df_coef = df_coef[best_cols].T\n",
    "df_coef = df_coef.rename(index=str, columns={i:v for i,v in enumerate(newsgroups_train.target_names)})\n",
    "\n",
    "print df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anagram vectorizer chosen: (2, 2)\n",
      "Data prepared.\n"
     ]
    }
   ],
   "source": [
    "train_data_df2, dev_data_df2 = vectorize_data2df(train_data,dev_data,(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "[(u'are you', 0.33886739304914343), (u'is not', 0.34483923353425766), (u'in this', 0.3524820518892682), (u'claim that', 0.3844124388257842), (u'cheers kent', 0.40272102275620614)] \n",
      "\n",
      "comp.graphics\n",
      "[(u'comp graphics', 0.5106692751744454), (u'is there', 0.531400622844133), (u'out there', 0.5659657519588308), (u'in advance', 0.6334320213442226), (u'looking for', 0.8201783029025946)] \n",
      "\n",
      "sci.space\n",
      "[(u'it was', 0.39891986676838515), (u'and such', 0.44899188575889376), (u'sci space', 0.4549261111128839), (u'the space', 0.6457584242300268), (u'the moon', 0.6593809004799214)] \n",
      "\n",
      "talk.religion.misc\n",
      "[(u'the word', 0.3121599267216939), (u'but he', 0.32389846098503117), (u'with you', 0.3311586555264025), (u'the fbi', 0.40292573889216404), (u'cheers kent', 0.4105155201815146)] \n",
      "\n",
      "               alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "are you           0.338867      -0.179555  -0.084510           -0.199806\n",
      "is not            0.344839      -0.172796  -0.337052            0.033743\n",
      "in this           0.352482      -0.013114  -0.359461           -0.082398\n",
      "claim that        0.384412      -0.131778  -0.182045           -0.079203\n",
      "cheers kent       0.402721      -0.474785  -0.458470            0.410516\n",
      "comp graphics    -0.194853       0.510669  -0.256416           -0.171966\n",
      "is there         -0.230449       0.531401  -0.339330           -0.163074\n",
      "out there        -0.209386       0.565966  -0.344805           -0.202674\n",
      "in advance       -0.343978       0.633432  -0.318699           -0.304098\n",
      "looking for      -0.467358       0.820178  -0.360398           -0.409295\n",
      "it was           -0.135249      -0.236477   0.398920           -0.215209\n",
      "and such         -0.151653      -0.249952   0.448992           -0.164463\n",
      "sci space        -0.183277      -0.246964   0.454926           -0.155422\n",
      "the space        -0.205710      -0.371378   0.645758           -0.206645\n",
      "the moon         -0.273941      -0.373724   0.659381           -0.177591\n",
      "the word          0.041313      -0.177893  -0.196901            0.312160\n",
      "but he           -0.114164      -0.143284  -0.088022            0.323898\n",
      "with you         -0.152193       0.033041  -0.239821            0.331159\n",
      "the fbi          -0.090311      -0.145645  -0.208189            0.402926\n",
      "cheers kent       0.402721      -0.474785  -0.458470            0.410516\n"
     ]
    }
   ],
   "source": [
    "cls = LogisticRegression(C = log_bestp)\n",
    "cls.fit(train_data_df2, train_labels)\n",
    "\n",
    "coefficients = cls.coef_\n",
    "df_coef = pd.DataFrame(coefficients, columns = train_data_df2.columns.values)\n",
    "\n",
    "best_cols = []\n",
    "\n",
    "for i,thelist in enumerate(coefficients):\n",
    "    print newsgroups_train.target_names[i]\n",
    "    \n",
    "    top_list = sorted(zip(train_data_df2.columns.values, cls.coef_[i]),key=lambda tup: tup[1])[-5:]\n",
    "    print top_list,'\\n'\n",
    "    \n",
    "    for (name, value) in top_list:\n",
    "        best_cols.append(name)\n",
    "        \n",
    "df_coef = df_coef[best_cols].T\n",
    "df_coef = df_coef.rename(index=str, columns={i:v for i,v in enumerate(newsgroups_train.target_names)})\n",
    "\n",
    "print df_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a custom pre-processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Transforming Train Data \n",
      "\n",
      "Normal vectorizer chosen\n",
      "Data prepared.\n",
      "\n",
      " Transforming Dev Data \n",
      "\n",
      "Normal vectorizer chosen\n",
      "Adding preprocessing function.\n",
      "Data prepared.\n",
      "\n",
      " Transformation Successful \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make Lowercase\n",
    "def lowercase(s):\n",
    "    return s.lower()\n",
    "\n",
    "# Replace all numbers with 1\n",
    "# This regex is from here: https://stackoverflow.com/questions/1043619/get-numbers-from-string-with-regex\n",
    "def replace_numbers(s):\n",
    "    return re.sub('\\d+', 'number', s)\n",
    "\n",
    "# Replace non-letter symbols. \n",
    "# Looking at the data, the biggest symbol seems to be \"_\". Therefore only replacing that one. Tried the below but it made more features somehow.\n",
    "# return re.sub('[^a-zA-Z0-9 ]','',s)\n",
    "def replace_non_letter(s):\n",
    "    return re.sub('[_]','',s)\n",
    "\n",
    "# Erase words beyond 5 characters\n",
    "def erase_long_words(s):\n",
    "    corpus = ''\n",
    "    for word in s.strip().split(' '):\n",
    "        word = re.sub('\\b[a-zA-Z]{1,4}\\b','',s)\n",
    "        corpus += word + ' '\n",
    "        \n",
    "    return corpus\n",
    "\n",
    "# Lemming the words\n",
    "def lem_word(s):\n",
    "    lem = WordNetLemmatizer()\n",
    "    corpus = ''\n",
    "    for word in s.strip().split(' '):\n",
    "        word = lem.lemmatize(word)\n",
    "    \n",
    "        corpus += word + ' '\n",
    "        \n",
    "    return corpus\n",
    "\n",
    "# Stemming the words\n",
    "def stem_words(s):\n",
    "    st = SnowballStemmer('english')\n",
    "    corpus = ''\n",
    "    for word in s.strip().split(' '):\n",
    "        word = st.stem(word)\n",
    "    \n",
    "        corpus += word + ' '\n",
    "        \n",
    "    return corpus\n",
    "\n",
    "# Finds the closests word in word_list and then replaces\n",
    "def replace_with_closest(s):\n",
    "    match = difflib.get_close_matches(s,word_list, cutoff=0.8, n=1)\n",
    "    if not match:\n",
    "        return s\n",
    "    else:\n",
    "        return match[0]\n",
    "\n",
    "# Putting all the functions together. If commented out, that function made it worse.\n",
    "def better_preprocessor(s):\n",
    "    s = lowercase(s)\n",
    "    s = replace_non_letter(s)\n",
    "    s = replace_numbers(s)\n",
    "    s = lem_word(s)\n",
    "    #s = replace_with_closest(s)\n",
    "    #s = stem_words(s)\n",
    "    #s = erase_long_words(s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "# Preparing the data. \n",
    "\n",
    "# Make the normal data\n",
    "print '\\n Transforming Train Data \\n'\n",
    "train_data_df, dev_data_df = vectorize_data2df(train_data,dev_data,(1,1))\n",
    "\n",
    "# Make the preprocessed data\n",
    "print '\\n Transforming Dev Data \\n'\n",
    "train_data_df_proc, dev_data_df_proc = vectorize_data2df(train_data,dev_data,(1,1),better_preprocessor, show_features=0)\n",
    "\n",
    "print '\\n Transformation Successful \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The normal stats are:\n",
      "The accuracy was 0.7144970414201184\n",
      "The f1 score is 0.7075749110117433 \n",
      "\n",
      "After processing the stats are:\n",
      "The accuracy was 0.7174556213017751\n",
      "The f1 score is 0.7112445521163012 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def doLogistic_custompre(train, dev):\n",
    "    \n",
    "    cls = LogisticRegression(C = log_bestp)\n",
    "    cls.fit(train, train_labels)\n",
    "\n",
    "    predictions = cls.predict(dev)\n",
    "    cls_reports = classification_report(dev_labels, predictions)\n",
    "    accuracy = accuracy_score(dev_labels, predictions)\n",
    "    f1 = f1_score(dev_labels, predictions, average = 'weighted')\n",
    "\n",
    "    print 'The accuracy was', accuracy\n",
    "    print 'The f1 score is', f1, '\\n'\n",
    "\n",
    "print 'The normal stats are:'\n",
    "doLogistic_custompre(train_data_df, dev_data_df)\n",
    "print 'After processing the stats are:'\n",
    "doLogistic_custompre(train_data_df_proc, dev_data_df_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the accuracy and f1 improved a little with the changes made. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using L1 Regularization to reduce sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below function is for logistic that has a parameter for setting penalty\n",
    "def doLogistic_L1L2(train, dev, penalty = 'l2', C_val = log_bestp):\n",
    "        \n",
    "    #print 'Initial Train shape',train.shape,'Dev shape',dev.shape\n",
    "    \n",
    "    cls = LogisticRegression(C = C_val, penalty = penalty)\n",
    "    cls.fit(train, train_labels)\n",
    "    \n",
    "    df = pd.DataFrame(cls.coef_, columns = train.columns.values)\n",
    "\n",
    "    return df\n",
    "\n",
    "# The below is to count non-zero features. \n",
    "def count_nonzero(df):\n",
    "    count = 0\n",
    "    \n",
    "    for column in df.columns.values:\n",
    "        if df[column].sum() != 0:\n",
    "            count += 1\n",
    "            \n",
    "    df = df.loc[:, (df != 0).any(axis=0)]\n",
    "            \n",
    "    return count, df.columns.values\n",
    "\n",
    "# The below is to retrain the data with the addition of adjusting the C parameter and making the graph at the end. \n",
    "def doLogistic_retrain(train,dev,penalty = 'l2', keep = None):\n",
    "    \n",
    "    if isinstance(keep, list):\n",
    "        dev = dev[keep]\n",
    "        train = train[keep]\n",
    "                \n",
    "    vocab_accuracy = {}\n",
    "    \n",
    "    cls = LogisticRegression(C = log_bestp, penalty = penalty, tol=0.01)\n",
    "    cls.fit(train, train_labels)\n",
    "\n",
    "    predictions = cls.predict(dev)\n",
    "    accuracy = accuracy_score(dev_labels, predictions)\n",
    "    \n",
    "    return accuracy \n",
    "\n",
    "\n",
    "# Keep this random seed here to make comparison easier.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Transforming Train Data \n",
      "\n",
      "Normal vectorizer chosen\n",
      "Data prepared.\n",
      "The number of features that have non-zero weights for L2 is 26879 whereas for L1 it is 337\n",
      "The retrained accuracy is 0.6760355029585798\n"
     ]
    }
   ],
   "source": [
    "# Make the normal data\n",
    "print '\\n Transforming Train Data \\n'\n",
    "train_data_df, dev_data_df = vectorize_data2df(train_data,dev_data,(1,1))\n",
    "\n",
    "# Executing the analysis and making the graph. \n",
    "coef_l2 = doLogistic_L1L2(train_data_df, dev_data_df, 'l2', log_bestp)\n",
    "coef_l2_count, L2_col_names  = count_nonzero(coef_l2)\n",
    "\n",
    "coef_l1 = doLogistic_L1L2(train_data_df, dev_data_df, 'l1', log_bestp)\n",
    "coef_l1_count, L1_col_names = count_nonzero(coef_l1)\n",
    "\n",
    "print 'The number of features that have non-zero weights for L2 is', coef_l2_count, 'whereas for L1 it is', coef_l1_count\n",
    "print 'The retrained accuracy is', doLogistic_retrain(train_data_df, dev_data_df, keep = L1_col_names.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TFIDF instead of Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal vectorizer chosen\n",
      "The accuracy was 0.7633136094674556 \n",
      "\n",
      "\n",
      "#############################\n",
      "Example of Poorly Predicted Document 1 Pred: comp.graphics True: talk.religion.misc \n",
      "#############################\n",
      "\n",
      "I am pleased to announce that a *revised version* of _The Easy-to-Read Book\n",
      "of Mormon_ (former title: _Mormon's Book_) by Lynn Matthews Anderson is now\n",
      "available through anonymous ftp (see information below). In addition to the\n",
      "change in title, the revised ETR BOM has been shortened by several pages\n",
      "(eliminating many extraneous \"that's\" and \"of's\"), and many (minor) errors\n",
      "have been corrected. This release includes a simplified Joseph Smith Story,\n",
      "testimonies of the three and eight witnesses, and a \"Words-to-Know\"\n",
      "glossary.\n",
      "\n",
      "As with the previous announcement, readers are reminded that this is a\n",
      "not-for-profit endeavor. This is a copyrighted work, but people are welcome\n",
      "to make *verbatim* copies for personal use. People can recuperate the\n",
      "actual costs of printing (paper, copy center charges), but may not charge\n",
      "anything for their time in making copies, or in any way realize a profit\n",
      "from the use of this book. See the permissions notice in the book itself\n",
      "for the precise terms.\n",
      "\n",
      "Negotiations are currently underway with a Mormon publisher vis-a-vis the\n",
      "printing and distribution of bound books. (Sorry, I'm out of the wire-bound\n",
      "\"first editions.\") I will make another announcement about the availability\n",
      "of printed copies once everything has been worked out.\n",
      "\n",
      "FTP information: connect via anonymous ftp to carnot.itc.cmu.edu, then \"cd\n",
      "pub\" (you won't see anything at all until you do).\n",
      "\n",
      "\"The Easy-to-Read Book of Mormon\" is currently available in postscript and\n",
      "RTF (rich text format). (ASCII, LaTeX, and other versions can be made\n",
      "available; contact dba@andrew.cmu.edu for details.) You should be able to\n",
      "print the postscript file on any postscript printer (such as an Apple\n",
      "Laserwriter); let dba know if you have any difficulties. (The postscript in\n",
      "the last release had problems on some printers; this time it should work\n",
      "better.) RTF is a standard document interchange format that can be read in\n",
      "by a number of word processors, including Microsoft Word for both the\n",
      "Macintosh and Windows. If you don't have a postscript printer, you may be\n",
      "able to use the RTF file to print out a copy of the book.\n",
      "\n",
      "-r--r--r--  1 dba                   1984742 Apr 27 13:12 etrbom.ps\n",
      "-r--r--r--  1 dba                   1209071 Apr 27 13:13 etrbom.rtf\n",
      "\n",
      "For more information about how this project came about, please refer to my\n",
      "article in the current issue of _Sunstone_, entitled \"Delighting in\n",
      "Plainness: Issues Surrounding a Simple Modern English Book of Mormon.\"\n",
      "\n",
      "Send all inquiries and comments to:\n",
      "\n",
      "    Lynn Matthews Anderson\n",
      "    5806 Hampton Street\n",
      "    Pittsburgh, PA 15206\n",
      "\n",
      "#############################\n",
      "Example of Poorly Predicted Document 2 Pred: comp.graphics True: talk.religion.misc \n",
      "#############################\n",
      "\n",
      "Can anyone provide me a ftp site where I can obtain a online version\n",
      "of the Book of Mormon. Please email the internet address if possible.\n",
      "\n",
      "#############################\n",
      "Example of Poorly Predicted Document 3 Pred: talk.religion.misc True: alt.atheism \n",
      "#############################\n",
      "\n",
      "\n",
      "The 24 children were, of course, killed by a lone gunman in a second story\n",
      "window, who fired eight bullets in the space of two seconds...\n",
      "\n",
      "\n",
      "#############################\n",
      "Example of Poorly Predicted Document 4 Pred: alt.atheism True: talk.religion.misc \n",
      "#############################\n",
      "\n",
      "In <1ren9a$94q@morrow.stanford.edu> salem@pangea.Stanford.EDU (Bruce Salem) \n",
      "\n",
      "\n",
      "\n",
      "This brings up another something I have never understood.  I asked this once\n",
      "before and got a few interesting responses, but somehow didn't seem satisfied.\n",
      "Why would the NT NOT be considered a good source.  This might be a \n",
      "literary/historical question, but when I studied history I always looked for \n",
      "firsthand original sources to write my papers.  If the topic was on Mr. X, I \n",
      "looked to see if Mr. X wrote anything about it.  If the topic was on a group, \n",
      "look for the group, etc.  If the topic is on Mr. X, and Mr. X did not write \n",
      "anything about it, (barring the theistic response about the Bible being \n",
      "divinely inspired which I can't adequately argue), wouldn't we look for people\n",
      "who ate, worked, walked, talked, etc. with him?  If someone was at an event \n",
      "wouldn't they be a better \"reporter\" than someone who heard about it second \n",
      "hand?  I guess isn't firsthand better than second hand.  I know, there is bias,\n",
      "and winners writing history, but doesn't the principle of firsthand being best\n",
      "still apply?\n",
      "\n",
      "MAC\n",
      "--\n",
      "****************************************************************\n",
      "                                                    Michael A. Cobb\n",
      " \"...and I won't raise taxes on the middle     University of Illinois\n",
      "    class to pay for my programs.\"                 Champaign-Urbana\n",
      "          -Bill Clinton 3rd Debate             cobb@alexia.lis.uiuc.edu\n",
      "\n",
      "#############################\n",
      "Example of Poorly Predicted Document 5 Pred: talk.religion.misc True: alt.atheism \n",
      "#############################\n",
      "\n",
      "With the Southern Baptist Convention convening this June to consider\n",
      "the charges that Freemasonry is incompatible with christianity, I thought\n",
      "the following quotes by Mr. James Holly, the Anti-Masonic Flag Carrier,\n",
      "would amuse you all...\n",
      "\n",
      "     The following passages are exact quotes from \"The Southern \n",
      "Baptist Convention and Freemasonry\" by James L. Holly, M.D., President\n",
      "of Mission and Ministry To Men, Inc., 550 N 10th St., Beaumont, TX \n",
      "77706. \n",
      " \n",
      "     The inside cover of the book states: \"Mission & Ministry to Men, \n",
      "Inc. hereby grants permission for the reproduction of part or all of \n",
      "this booklet with two provisions: one, the material is not changed and\n",
      "two, the source is identified.\" I have followed these provisions. \n",
      "  \n",
      "     \"Freemasonry is one of the allies of the Devil\" Page iv. \n",
      " \n",
      "     \"The issue here is not moderate or conservative, the issue is God\n",
      "and the Devil\" Page vi.\" \n",
      " \n",
      "     \"It is worthwhile to remember that the formulators of public \n",
      "school education in America were Freemasons\" Page 29. \n",
      " \n",
      "     \"Jesus Christ never commanded toleration as a motive for His \n",
      "disciples, and toleration is the antithesis of the Christian message.\"\n",
      "Page 30. \n",
      " \n",
      "     \"The central dynamic of the Freemason drive for world unity \n",
      "through fraternity, liberty and equality is toleration. This is seen \n",
      "in the writings of the 'great' writers of Freemasonry\". Page 31. \n",
      " \n",
      "     \"He [Jesus Christ] established the most sectarian of all possible \n",
      "faiths.\" Page 37. \n",
      " \n",
      "     \"For narrowness and sectarianism, there is no equal to the Lord \n",
      "Jesus Christ\". Page 40. \n",
      " \n",
      "     \"What seems so right in the interest of toleration and its \n",
      "cousins-liberty, equality and fraternity-is actually one of the \n",
      "subtlest lies of the 'father of lies.'\" Page 40. \n",
      " \n",
      "     \"The Southern Baptist Convention has many churches which were \n",
      "founded in the Lodge and which have corner stones dedicated by the \n",
      "Lodge. Each of these churches should hold public ceremonies of \n",
      "repentance and of praying the blood and the Name of the Lord Jesus \n",
      "Christ over the church and renouncing the oaths taken at the \n",
      "dedication of the church and/or building.\" Page 53-54.  \n",
      " \n"
     ]
    }
   ],
   "source": [
    "def vectorize_tfidf_data2df(train, dev, gram, preprocessor = None, stop_words = None, sublinear_tf = None, min_df = None, show_features = 0):\n",
    "    \n",
    "    if gram == (1,1):\n",
    "        print 'Normal vectorizer chosen'\n",
    "    else:\n",
    "        print 'Anagram vectorizer chosen:', gram\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(ngram_range = gram, analyzer =\"word\")\n",
    "        \n",
    "    if preprocessor != None:\n",
    "        print 'Adding preprocessing function.'\n",
    "        vectorizer.set_params(preprocessor=preprocessor)\n",
    "    \n",
    "    if stop_words != None:\n",
    "        print 'Adding stop_words function.'\n",
    "        vectorizer.set_params(stop_words=stop_words)   \n",
    "        \n",
    "    if sublinear_tf != None:\n",
    "        print 'Adding sublinear_df function.'\n",
    "        vectorizer.set_params(sublinear_tf=sublinear_tf)   \n",
    "        \n",
    "    if min_df != None:\n",
    "        print 'Adding min_df function.'\n",
    "        vectorizer.set_params(min_df=min_df)   \n",
    "            \n",
    "    train_matrix = vectorizer.fit_transform(train).todense()\n",
    "    dev_matrix = vectorizer.transform(dev).todense()\n",
    "    \n",
    "    columns = vectorizer.get_feature_names()\n",
    "    \n",
    "    if show_features == 1:\n",
    "        print columns\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    df_train = pd.DataFrame(data=train_matrix, columns=columns)\n",
    "    df_dev = pd.DataFrame(data=dev_matrix, columns=columns)\n",
    "    \n",
    "    return df_train, df_dev\n",
    "\n",
    "# Defining the logistic function plus the outputs required. \n",
    "\n",
    "def doLogistic_tfidf(train, dev):\n",
    "    \n",
    "    cls = LogisticRegression(C = 100)\n",
    "    cls.fit(train, train_labels)\n",
    "    \n",
    "    pred_prob = pd.DataFrame(cls.predict_proba(dev))\n",
    "    pred_prob['max'] = pred_prob.max(axis=1)\n",
    "    pred_prob['pred'] = cls.predict(dev)\n",
    "    pred_prob['labels'] = dev_labels\n",
    "    pred_prob['label_prob'] = pred_prob.lookup(pred_prob.index, pred_prob.labels)\n",
    "    pred_prob['R ratio'] = pred_prob['max']/pred_prob['label_prob']\n",
    "\n",
    "    pred_prob = pred_prob.sort_values(by=['R ratio'], ascending=False)\n",
    "    \n",
    "    predictions = cls.predict(dev)\n",
    "    accuracy = accuracy_score(dev_labels, predictions)\n",
    "    \n",
    "    print 'The accuracy was', accuracy, '\\n'\n",
    "\n",
    "    for i,(v,l,p) in enumerate(zip(pred_prob.head(5).index,pred_prob['labels'].head(5),pred_prob['pred'].head(5))):\n",
    "        print '\\n#############################\\nExample of Poorly Predicted Document', i+1,'Pred:',newsgroups_train.target_names[p],'True:',newsgroups_train.target_names[l], '\\n#############################\\n'\n",
    "        print dev_data[v]\n",
    "    \n",
    "\n",
    "# Running the above functions. \n",
    "    \n",
    "train_data_df, dev_data_df = vectorize_tfidf_data2df(train_data, dev_data, (1,1))\n",
    "\n",
    "doLogistic_tfidf(train_data_df, dev_data_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with the original method of using CountVectorizer to the point here using TfidfVectorizer, we have essentially improved from approximately 0.71 to approximately 0.76 for both accuracy and f1 score. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel_py2",
   "language": "python",
   "name": "ipykernel_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
